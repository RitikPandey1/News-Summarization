{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input,  LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from Attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./news_summary.csv\",nrows=70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)\n",
    "    return(\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in dataset['text']:\n",
    "    cleaned_text.append(text_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headline_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower() \n",
    "    tokens = newString.split()\n",
    "    newString = ''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString \n",
    "\n",
    "#Call the above function\n",
    "cleaned_headline = []\n",
    "for t in dataset['headlines']:\n",
    "    cleaned_headline.append(headline_cleaner(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cleaned_text']=cleaned_text\n",
    "dataset['cleaned_headline']=cleaned_headline\n",
    "dataset['cleaned_headline'].replace('', np.nan, inplace=True)\n",
    "dataset.dropna(axis=0,inplace=True)\n",
    "\n",
    "dataset['cleaned_headline'] = dataset['cleaned_headline'].apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "\n",
    "word_count_text = []\n",
    "word_count_headline = []\n",
    "\n",
    "for i in dataset['cleaned_text']:\n",
    "    word_count_text.append(len(i.split(' ')))\n",
    "\n",
    "for i in dataset['cleaned_headline']:\n",
    "    word_count_headline.append(len(i.split(' ')))\n",
    "\n",
    "\n",
    "max_len_text = max(word_count_text)\n",
    "max_len_headline = max(word_count_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(dataset['cleaned_text'],dataset['cleaned_headline'],test_size=0.1,random_state=0,shuffle=True) \n",
    "\n",
    "\n",
    "# tokenization\n",
    "# for text\n",
    "x_token = Tokenizer()\n",
    "# fit_on_texts method build word_index =>  creates the vocabulary index based on word frequency\n",
    "x_token.fit_on_texts(list(x_tr))\n",
    "\n",
    "x_tr = x_token.texts_to_sequences(x_tr)\n",
    "x_val = x_token.texts_to_sequences(x_val)\n",
    "\n",
    "x_tr = pad_sequences(x_tr,maxlen=max_len_text,padding='post')\n",
    "x_val = pad_sequences(x_val,maxlen=max_len_text,padding='post')\n",
    "\n",
    "x_voc_size = len(x_token.word_index) + 1\n",
    "\n",
    "# for headlines\n",
    "y_token = Tokenizer()\n",
    "y_token.fit_on_texts(list(y_tr))\n",
    "\n",
    "y_tr = y_token.texts_to_sequences(y_tr)\n",
    "y_val = y_token.texts_to_sequences(y_val)\n",
    "\n",
    "y_tr = pad_sequences(y_tr,maxlen=max_len_headline,padding='post')\n",
    "y_val = pad_sequences(y_val,maxlen=max_len_headline,padding='post')\n",
    "\n",
    "y_voc_size = len(y_token.word_index) + 1\n",
    "\n",
    "x_tr.shape,y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "k.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "#encoder\n",
    "enc_inp = Input(shape=(max_len_text,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb = Embedding(x_voc_size,embedding_dim,trainable=True)(enc_inp)\n",
    "# bidirectional lstm\n",
    "encoder_1 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True))\n",
    "enc_out_1, frw_h1, frw_c1, bck_h1,bck_c1 = encoder_1(enc_emb)\n",
    "\n",
    "encoder_2 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True))\n",
    "enc_out_2, frw_h2, frw_c2, bck_h2,bck_c2 = encoder_2(enc_out_1)\n",
    "\n",
    "encoder = Bidirectional(LSTM(latent_dim,return_state=True, return_sequences=True))\n",
    "enc_out, frw_h, frw_c, bck_h, bck_c  = encoder(enc_out_2)\n",
    "state_h = Concatenate()([frw_h,bck_h])\n",
    "state_c = Concatenate()([frw_c,bck_c])\n",
    "encoder_states = [state_h,state_c]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9973c6938f4aad7dcfd49854c1e0cc6453bbf87fb3680ba791d7b26d8dd01db7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
